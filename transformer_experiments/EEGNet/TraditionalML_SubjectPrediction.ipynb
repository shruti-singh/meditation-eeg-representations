{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " Sample script using EEGNet to classify Event-Related Potential (ERP) EEG data\n",
    " from a four-class classification task, using the sample dataset provided in\n",
    " the MNE [1, 2] package:\n",
    "     https://martinos.org/mne/stable/manual/sample_dataset.html#ch-sample-data\n",
    "   \n",
    " The four classes used from this dataset are:\n",
    "     LA: Left-ear auditory stimulation\n",
    "     RA: Right-ear auditory stimulation\n",
    "     LV: Left visual field stimulation\n",
    "     RV: Right visual field stimulation\n",
    "\n",
    " The code to process, filter and epoch the data are originally from Alexandre\n",
    " Barachant's PyRiemann [3] package, released under the BSD 3-clause. A copy of \n",
    " the BSD 3-clause license has been provided together with this software to \n",
    " comply with software licensing requirements. \n",
    " \n",
    " When you first run this script, MNE will download the dataset and prompt you\n",
    " to confirm the download location (defaults to ~/mne_data). Follow the prompts\n",
    " to continue. The dataset size is approx. 1.5GB download. \n",
    " \n",
    " For comparative purposes you can also compare EEGNet performance to using \n",
    " Riemannian geometric approaches with xDAWN spatial filtering [4-8] using \n",
    " PyRiemann (code provided below).\n",
    "\n",
    " [1] A. Gramfort, M. Luessi, E. Larson, D. Engemann, D. Strohmeier, C. Brodbeck,\n",
    "     L. Parkkonen, M. Hämäläinen, MNE software for processing MEG and EEG data, \n",
    "     NeuroImage, Volume 86, 1 February 2014, Pages 446-460, ISSN 1053-8119.\n",
    "\n",
    " [2] A. Gramfort, M. Luessi, E. Larson, D. Engemann, D. Strohmeier, C. Brodbeck, \n",
    "     R. Goj, M. Jas, T. Brooks, L. Parkkonen, M. Hämäläinen, MEG and EEG data \n",
    "     analysis with MNE-Python, Frontiers in Neuroscience, Volume 7, 2013.\n",
    "\n",
    " [3] https://github.com/alexandrebarachant/pyRiemann. \n",
    "\n",
    " [4] A. Barachant, M. Congedo ,\"A Plug&Play P300 BCI Using Information Geometry\"\n",
    "     arXiv:1409.0107. link\n",
    "\n",
    " [5] M. Congedo, A. Barachant, A. Andreev ,\"A New generation of Brain-Computer \n",
    "     Interface Based on Riemannian Geometry\", arXiv: 1310.8115.\n",
    "\n",
    " [6] A. Barachant and S. Bonnet, \"Channel selection procedure using riemannian \n",
    "     distance for BCI applications,\" in 2011 5th International IEEE/EMBS \n",
    "     Conference on Neural Engineering (NER), 2011, 348-351.\n",
    "\n",
    " [7] A. Barachant, S. Bonnet, M. Congedo and C. Jutten, “Multiclass \n",
    "     Brain-Computer Interface Classification by Riemannian Geometry,” in IEEE \n",
    "     Transactions on Biomedical Engineering, vol. 59, no. 4, p. 920-928, 2012.\n",
    "\n",
    " [8] A. Barachant, S. Bonnet, M. Congedo and C. Jutten, “Classification of \n",
    "     covariance matrices using a Riemannian-based kernel for BCI applications“, \n",
    "     in NeuroComputing, vol. 112, p. 172-178, 2013.\n",
    "\n",
    "\n",
    " Portions of this project are works of the United States Government and are not\n",
    " subject to domestic copyright protection under 17 USC Sec. 105.  Those \n",
    " portions are released world-wide under the terms of the Creative Commons Zero \n",
    " 1.0 (CC0) license.  \n",
    " \n",
    " Other portions of this project are subject to domestic copyright protection \n",
    " under 17 USC Sec. 105.  Those portions are licensed under the Apache 2.0 \n",
    " license.  The complete text of the license governing this material is in \n",
    " the file labeled LICENSE.TXT that is a part of this project's official \n",
    " distribution. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# mne imports\n",
    "import mne\n",
    "from mne import io\n",
    "from mne.datasets import sample\n",
    "\n",
    "# EEGNet-specific imports\n",
    "from EEGModels import EEGNet\n",
    "from tensorflow.keras import utils as np_utils\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# PyRiemann imports\n",
    "from pyriemann.estimation import XdawnCovariances\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "from pyriemann.utils.viz import plot_confusion_matrix\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# tools for plotting confusion matrices\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while the default tensorflow ordering is 'channels_last' we set it here\n",
    "# to be explicit in case if the user has changed the default ordering\n",
    "K.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing & results----------------\n",
    "from sklearn.model_selection import train_test_split, cross_validate, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Models-------------------------\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "import sklearn.gaussian_process.kernels as kls\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, BaggingClassifier, ExtraTreesClassifier\n",
    "\n",
    "# General purpose\n",
    "import re\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(user_split = True):\n",
    "    X_train = None\n",
    "    y_train_medid = None\n",
    "    y_train_subjid = None\n",
    "    X_val = None\n",
    "    y_val_medid = None\n",
    "    y_val_subjid = None\n",
    "    X_test = None\n",
    "    y_test_medid = None\n",
    "    y_test_subjid = None\n",
    "\n",
    "    # user_split: Determines the creation of train/val/test set. In user_split, test/val users are never seen during the train. time_split randomly splits each user's chunks into train/val/test.\n",
    "    if user_split:\n",
    "        data_file_path = '../../data/Meditation/user_based_splits.pkl'\n",
    "    else:\n",
    "        data_file_path = '../../data/Meditation/time_based_splits.pkl'\n",
    "\n",
    "    with open(data_file_path, 'rb') as f:\n",
    "        all_data_splits = pickle.load(f)\n",
    "\n",
    "        X_train = all_data_splits['train']['x']\n",
    "        y_train_medid = all_data_splits['train']['y_med']\n",
    "        y_train_subjid = all_data_splits['train']['y_subj']\n",
    "\n",
    "        X_val = all_data_splits['val']['x']\n",
    "        y_val_medid = all_data_splits['val']['y_med']\n",
    "        y_val_subjid = all_data_splits['val']['y_subj']\n",
    "\n",
    "        X_test = all_data_splits['test']['x']\n",
    "        y_test_medid = all_data_splits['test']['y_med']\n",
    "        y_test_subjid = all_data_splits['test']['y_subj']\n",
    "\n",
    "    return X_train, y_train_medid, y_train_subjid, X_val, y_val_medid, y_val_subjid, X_test, y_test_medid, y_test_subjid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_dict = {\n",
    "#     'DecisionTree': {\"model\": DecisionTreeClassifier(random_state=42), \"params\": {'max_depth': list(range(2, 16, 3))}},\n",
    "#     'RandomForest': {\"model\": RandomForestClassifier(random_state=42),\n",
    "#                      \"params\": {'n_estimators': list(range(5, 100, 5)), 'max_depth': list(range(2, 16))}},\n",
    "#     'LogisticR_L1': {\"model\": LogisticRegression(random_state=42, max_iter=500),\n",
    "#                      \"params\": {'penalty': ['l1'], 'solver': ['liblinear', 'saga']}},\n",
    "#     'LogisticR_L2': {\"model\": LogisticRegression(random_state=42, max_iter=1000),\n",
    "#                      \"params\": {'penalty': ['l2'], 'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}},\n",
    "#     'LogisticR': {\"model\": LogisticRegression(random_state=42, max_iter=500),\n",
    "#                   \"params\": {'penalty': ['none'], 'solver': ['newton-cg', 'lbfgs', 'sag', 'saga']}},\n",
    "#     'RidgeClf': {\"model\": RidgeClassifier(max_iter=1000), \"params\": {}},\n",
    "#     'SVC_linear': {\"model\": SVC(random_state=42), \"params\": {'kernel': ['linear'], \n",
    "#                                                              'C': [0.5, 1.0, 1.5, 2.0, 2.5]}},\n",
    "#     'SVC_poly': {\"model\": SVC(random_state=42),\n",
    "#                  \"params\": {'kernel': ['poly'], 'degree': [3, 4, 5], 'gamma': ['scale', 'auto'], \n",
    "#                             'C': [0.5, 1.0, 1.5, 2.0, 2.5]}},\n",
    "#     'SVC_others': {\"model\": SVC(random_state=42), \"params\": {'kernel': ['rbf', 'sigmoid'], \n",
    "#                                                              'gamma': ['scale', 'auto'], \n",
    "#                                                              'C': [0.5, 1.0, 1.5, 2.0, 2.5]}},\n",
    "#     'GussianNB': {\"model\": GaussianNB(), \"params\": {}},\n",
    "#     'KNN': {\"model\": KNeighborsClassifier(), \"params\": {'n_neighbors': list(range(3, 30))}},\n",
    "#     'GaussianProcessClf': {\"model\": GaussianProcessClassifier(random_state=42, kernel=kls.RBF()), \"params\": {}},\n",
    "    'Bagging_SVC': {\"model\": BaggingClassifier(random_state=42), \"params\": {'n_estimators': list(range(5, 100, 20)),\n",
    "                                                                            'base_estimator': [SVC(kernel='linear'),\n",
    "                                                                                               SVC(kernel='poly',\n",
    "                                                                                                   degree=3,\n",
    "                                                                                                   gamma='scale')]}},\n",
    "    'BaggingDT': {\"model\": BaggingClassifier(random_state=42), \"params\": {'n_estimators': list(range(5, 100, 20)),\n",
    "                                                                          'base_estimator': [\n",
    "                                                                              DecisionTreeClassifier(random_state=42,\n",
    "                                                                                                     max_depth=2),\n",
    "                                                                              DecisionTreeClassifier(random_state=42,\n",
    "                                                                                                     max_depth=5),\n",
    "                                                                              DecisionTreeClassifier(random_state=42,\n",
    "                                                                                                     max_depth=10)]}},\n",
    "    'AdaBoost': {\"model\": AdaBoostClassifier(random_state=42), \"params\": {'n_estimators': list(range(5, 100, 20)),\n",
    "                                                                          'base_estimator': [DecisionTreeClassifier(\n",
    "                                                                                                 random_state=42,\n",
    "                                                                                                 max_depth=2),\n",
    "                                                                                             DecisionTreeClassifier(\n",
    "                                                                                                 random_state=42,\n",
    "                                                                                                 max_depth=5),\n",
    "                                                                                             DecisionTreeClassifier(\n",
    "                                                                                                 random_state=42,\n",
    "                                                                                                 max_depth=10)]}},\n",
    "    'ExtraTrees': {\"model\": ExtraTreesClassifier(random_state=42), \"params\": {'n_estimators': list(range(5, 105, 30)), \n",
    "                                                                              'max_depth': [2,5,10,15]}},\n",
    "    'MLP_l1': {\"model\": MLPClassifier(random_state=42), \"params\": {'hidden_layer_sizes': [(x,) for x in \n",
    "                                                                                          range(50, 600, 200)], \n",
    "                                                                  'activation': ['logistic', 'tanh', 'relu'],\n",
    "                                                                  'solver': ['adam', 'sgd'], 'early_stopping': \n",
    "                                                                   [True]}},\n",
    "    'MLP_l2': {\"model\": MLPClassifier(random_state=42), \"params\": {'hidden_layer_sizes': [(x, y) for x in \n",
    "                                                                                          range(50, 600, 200) \n",
    "                                                                                          for y in range(50, 360, 200)], \n",
    "                                                                  'activation': ['logistic', 'tanh', 'relu'],\n",
    "                                                                  'solver': ['adam', 'sgd'], 'early_stopping': \n",
    "                                                                                               [True]}},\n",
    "#     'MLP_l3': {\"model\": MLPClassifier(random_state=42), \"params\": {'hidden_layer_sizes': [(x, y, z) for x in \n",
    "#                                                                                           range(50, 600, 100) \n",
    "#                                                                                           for y in range(50, 600, 100)\n",
    "#                                                                                           for z in range(50, 360, 100)], \n",
    "#                                                                   'activation': ['logistic', 'tanh', 'relu'],\n",
    "#                                                                   'solver': ['adam', 'sgd'], 'early_stopping': \n",
    "#                                                                                                [True]}},\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_split   = False\n",
    "med_tech_clf = False\n",
    "\n",
    "X_train, y_train_medid, y_train_subjid, X_val, y_val_medid, y_val_subjid, X_test, y_test_medid, y_test_subjid = load_dataset(user_split=user_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking mean across each channel\n",
    "\n",
    "# X_train_list = []\n",
    "# X_val_list = []\n",
    "# X_test_list = []\n",
    "\n",
    "# for i in range(X_train.shape[0]):\n",
    "#     X_train_list.append(X_train[i,:,:].flatten())\n",
    "# X_train_list = np.array(X_train_list)\n",
    "# print(X_train.shape, X_train_list.shape)\n",
    "\n",
    "# for i in range(X_val.shape[0]):\n",
    "#     X_val_list.append(X_val[i,:,:].flatten())\n",
    "# X_val_list = np.array(X_val_list)\n",
    "# print(X_val.shape, X_val_list.shape)\n",
    "\n",
    "# for i in range(X_test.shape[0]):\n",
    "#     X_test_list.append(X_test[i,:,:].flatten())\n",
    "# X_test_list = np.array(X_test_list)\n",
    "# print(X_test.shape, X_test_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1954, 64, 2560) (1954, 64)\n",
      "(661, 64, 2560) (661, 64)\n",
      "(688, 64, 2560) (688, 64)\n"
     ]
    }
   ],
   "source": [
    "X_train_list = []\n",
    "X_val_list = []\n",
    "X_test_list = []\n",
    "\n",
    "for i in range(X_train.shape[0]):\n",
    "    X_train_list.append(np.mean(X_train[i,:,:], axis=1))\n",
    "X_train_list = np.array(X_train_list)\n",
    "print(X_train.shape, X_train_list.shape)\n",
    "\n",
    "for i in range(X_val.shape[0]):\n",
    "    X_val_list.append(np.mean(X_val[i,:,:], axis=1))\n",
    "X_val_list = np.array(X_val_list)\n",
    "print(X_val.shape, X_val_list.shape)\n",
    "\n",
    "for i in range(X_test.shape[0]):\n",
    "    X_test_list.append(np.mean(X_test[i,:,:], axis=1))\n",
    "X_test_list = np.array(X_test_list)\n",
    "print(X_test.shape, X_test_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_df = pd.DataFrame(X_train_list, columns=[str(x) for x in range(0, 163840)])\n",
    "# X_val_df = pd.DataFrame(X_val_list, columns=[str(x) for x in range(0, 163840)])\n",
    "# X_test_df = pd.DataFrame(X_test_list, columns=[str(x) for x in range(0, 163840)])\n",
    "\n",
    "# For keeping only 64 channels data\n",
    "X_train_df = pd.DataFrame(X_train_list, columns=[str(x) for x in range(0, 64)])\n",
    "X_val_df = pd.DataFrame(X_val_list, columns=[str(x) for x in range(0, 64)])\n",
    "X_test_df = pd.DataFrame(X_test_list, columns=[str(x) for x in range(0, 64)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging_SVC 0.9994871794871795 {'base_estimator': SVC(kernel='linear'), 'n_estimators': 5} 1.0\n",
      "BaggingDT 0.8403278903534657 {'base_estimator': DecisionTreeClassifier(max_depth=10, random_state=42), 'n_estimators': 85} 0.8199697428139183\n",
      "AdaBoost 0.5910892517542135 {'base_estimator': DecisionTreeClassifier(max_depth=10, random_state=42), 'n_estimators': 85} 0.5748865355521936\n",
      "ExtraTrees 1.0 {'max_depth': 15, 'n_estimators': 65} 0.9984871406959153\n",
      "MLP_l1 0.9979526526329596 {'activation': 'tanh', 'early_stopping': True, 'hidden_layer_sizes': (450,), 'solver': 'adam'} 0.9909228441754917\n",
      "MLP_l2 0.9989756705357727 {'activation': 'relu', 'early_stopping': True, 'hidden_layer_sizes': (250, 250), 'solver': 'adam'} 0.9909228441754917\n",
      "================================================================================\n",
      "SVC(C=0.5, kernel='linear', random_state=42)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        11\n",
      "           1       1.00      1.00      1.00        11\n",
      "           2       1.00      1.00      1.00        11\n",
      "           3       1.00      1.00      1.00        10\n",
      "           4       1.00      1.00      1.00        12\n",
      "           5       1.00      1.00      1.00        11\n",
      "           6       1.00      1.00      1.00        12\n",
      "           7       1.00      1.00      1.00        12\n",
      "           8       1.00      1.00      1.00        10\n",
      "           9       1.00      1.00      1.00        12\n",
      "          10       1.00      1.00      1.00        12\n",
      "          11       1.00      1.00      1.00        10\n",
      "          12       1.00      1.00      1.00        10\n",
      "          13       1.00      1.00      1.00        11\n",
      "          14       1.00      1.00      1.00        11\n",
      "          15       1.00      1.00      1.00        11\n",
      "          16       1.00      1.00      1.00        10\n",
      "          17       1.00      1.00      1.00        12\n",
      "          18       1.00      1.00      1.00        10\n",
      "          19       1.00      1.00      1.00        10\n",
      "          20       1.00      1.00      1.00        12\n",
      "          21       1.00      1.00      1.00         9\n",
      "          22       1.00      1.00      1.00        10\n",
      "          23       1.00      1.00      1.00         9\n",
      "          24       1.00      1.00      1.00         5\n",
      "          25       1.00      1.00      1.00        12\n",
      "          26       1.00      1.00      1.00        12\n",
      "          27       1.00      1.00      1.00        10\n",
      "          28       1.00      1.00      1.00        10\n",
      "          29       1.00      1.00      1.00        11\n",
      "          30       1.00      1.00      1.00        10\n",
      "          31       1.00      1.00      1.00        12\n",
      "          32       1.00      1.00      1.00        12\n",
      "          33       1.00      1.00      1.00        11\n",
      "          34       1.00      1.00      1.00        11\n",
      "          35       1.00      1.00      1.00        12\n",
      "          36       1.00      1.00      1.00        11\n",
      "          37       1.00      1.00      1.00        10\n",
      "          38       1.00      1.00      1.00        11\n",
      "          39       1.00      1.00      1.00        12\n",
      "          40       1.00      1.00      1.00        10\n",
      "          41       1.00      1.00      1.00        11\n",
      "          42       1.00      1.00      1.00        12\n",
      "          43       1.00      1.00      1.00        12\n",
      "          44       1.00      1.00      1.00         9\n",
      "          45       1.00      1.00      1.00        11\n",
      "          46       1.00      1.00      1.00        11\n",
      "          47       1.00      1.00      1.00        10\n",
      "          48       1.00      1.00      1.00        11\n",
      "          49       1.00      1.00      1.00        10\n",
      "          50       1.00      1.00      1.00        11\n",
      "          51       1.00      1.00      1.00        11\n",
      "          52       1.00      1.00      1.00        11\n",
      "          53       1.00      1.00      1.00        10\n",
      "          54       1.00      1.00      1.00        11\n",
      "          55       1.00      1.00      1.00         9\n",
      "          56       1.00      1.00      1.00        10\n",
      "          57       1.00      1.00      1.00        11\n",
      "          58       1.00      1.00      1.00        11\n",
      "          59       1.00      1.00      1.00         9\n",
      "          60       1.00      1.00      1.00        12\n",
      "          61       1.00      1.00      1.00        10\n",
      "          62       1.00      1.00      1.00        12\n",
      "          63       1.00      1.00      1.00        12\n",
      "\n",
      "    accuracy                           1.00       688\n",
      "   macro avg       1.00      1.00      1.00       688\n",
      "weighted avg       1.00      1.00      1.00       688\n",
      "\n",
      "Test acc: 1.0\n",
      "Weighted F1 score:  1.0\n"
     ]
    }
   ],
   "source": [
    "# X_train, y_train_medid, y_train_subjid, X_val, y_val_medid, y_val_subjid, X_test, y_test_medid, y_test_subjid\n",
    "# model_results = pd.DataFrame()\n",
    "# model_results['Train_Accuracy'] = None\n",
    "# model_results['Val_Accuracy'] = None\n",
    "# model_results['Test_Accuracy'] = None\n",
    "# model_results['best_params'] = None\n",
    "\n",
    "\n",
    "# best_clf_ours = None\n",
    "# best_clf_val = 0\n",
    "\n",
    "for clf_name, clf in clf_dict.items():\n",
    "    classifier = GridSearchCV(clf['model'], clf['params'], n_jobs=10)\n",
    "    classifier.fit(X_train_df, y_train_subjid)\n",
    "    best_model = classifier.best_estimator_\n",
    "    \n",
    "    y_predicted = best_model.predict(X_val_df)\n",
    "    val_acc = accuracy_score(y_val_subjid, y_predicted)\n",
    "    \n",
    "    print(clf_name, classifier.best_score_, classifier.best_params_, val_acc)\n",
    "    \n",
    "    if val_acc > best_clf_val:\n",
    "        best_clf_val = val_acc\n",
    "        best_clf_ours = best_model\n",
    "        \n",
    "    y_predicted = best_model.predict(X_test_df)\n",
    "    test_acc = accuracy_score(y_test_subjid, y_predicted)\n",
    "    \n",
    "    \n",
    "    model_results.loc[clf_name, ['Train_Accuracy', 'Val_Accuracy', 'Test_Accuracy', 'best_params']] = [classifier.best_score_, val_acc, test_acc, classifier.best_params_]\n",
    "    clsr = classification_report(y_test_subjid, y_predicted)\n",
    "\n",
    "print(\"================================================================================\")\n",
    "print(best_clf_ours)\n",
    "best_y_hat = best_clf_ours.predict(X_test_df)\n",
    "clsr = classification_report(y_test_subjid, best_y_hat)\n",
    "print(clsr)\n",
    "test_acc = accuracy_score(y_test_subjid, best_y_hat)\n",
    "print(\"Test acc:\", test_acc )\n",
    "print(\"Weighted F1 score: \", f1_score(y_test_subjid, best_y_hat, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train_Accuracy</th>\n",
       "      <th>Val_Accuracy</th>\n",
       "      <th>Test_Accuracy</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DecisionTree</th>\n",
       "      <td>0.313197</td>\n",
       "      <td>0.313162</td>\n",
       "      <td>0.313953</td>\n",
       "      <td>{'max_depth': 14}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForest</th>\n",
       "      <td>0.992326</td>\n",
       "      <td>0.987897</td>\n",
       "      <td>0.985465</td>\n",
       "      <td>{'max_depth': 15, 'n_estimators': 55}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticR_L1</th>\n",
       "      <td>0.998464</td>\n",
       "      <td>0.996974</td>\n",
       "      <td>0.994186</td>\n",
       "      <td>{'penalty': 'l1', 'solver': 'saga'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticR_L2</th>\n",
       "      <td>0.999487</td>\n",
       "      <td>0.996974</td>\n",
       "      <td>0.998547</td>\n",
       "      <td>{'penalty': 'l2', 'solver': 'liblinear'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticR</th>\n",
       "      <td>0.999487</td>\n",
       "      <td>0.998487</td>\n",
       "      <td>0.998547</td>\n",
       "      <td>{'penalty': 'none', 'solver': 'newton-cg'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RidgeClf</th>\n",
       "      <td>0.999488</td>\n",
       "      <td>0.998487</td>\n",
       "      <td>0.998547</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC_linear</th>\n",
       "      <td>0.999487</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'C': 0.5, 'kernel': 'linear'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC_poly</th>\n",
       "      <td>0.992838</td>\n",
       "      <td>0.992436</td>\n",
       "      <td>0.99564</td>\n",
       "      <td>{'C': 2.5, 'degree': 3, 'gamma': 'auto', 'kern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC_others</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.998487</td>\n",
       "      <td>0.998547</td>\n",
       "      <td>{'C': 1.0, 'gamma': 'scale', 'kernel': 'rbf'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GussianNB</th>\n",
       "      <td>0.998465</td>\n",
       "      <td>0.996974</td>\n",
       "      <td>0.998547</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'n_neighbors': 3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bagging_SVC</th>\n",
       "      <td>0.999487</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'base_estimator': SVC(kernel='linear'), 'n_es...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BaggingDT</th>\n",
       "      <td>0.840328</td>\n",
       "      <td>0.81997</td>\n",
       "      <td>0.819767</td>\n",
       "      <td>{'base_estimator': DecisionTreeClassifier(max_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoost</th>\n",
       "      <td>0.591089</td>\n",
       "      <td>0.574887</td>\n",
       "      <td>0.572674</td>\n",
       "      <td>{'base_estimator': DecisionTreeClassifier(max_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTrees</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.998487</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'max_depth': 15, 'n_estimators': 65}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_l1</th>\n",
       "      <td>0.997953</td>\n",
       "      <td>0.990923</td>\n",
       "      <td>0.992733</td>\n",
       "      <td>{'activation': 'tanh', 'early_stopping': True,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP_l2</th>\n",
       "      <td>0.998976</td>\n",
       "      <td>0.990923</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>{'activation': 'relu', 'early_stopping': True,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Train_Accuracy Val_Accuracy Test_Accuracy  \\\n",
       "DecisionTree       0.313197     0.313162      0.313953   \n",
       "RandomForest       0.992326     0.987897      0.985465   \n",
       "LogisticR_L1       0.998464     0.996974      0.994186   \n",
       "LogisticR_L2       0.999487     0.996974      0.998547   \n",
       "LogisticR          0.999487     0.998487      0.998547   \n",
       "RidgeClf           0.999488     0.998487      0.998547   \n",
       "SVC_linear         0.999487          1.0           1.0   \n",
       "SVC_poly           0.992838     0.992436       0.99564   \n",
       "SVC_others              1.0     0.998487      0.998547   \n",
       "GussianNB          0.998465     0.996974      0.998547   \n",
       "KNN                     1.0          1.0           1.0   \n",
       "Bagging_SVC        0.999487          1.0           1.0   \n",
       "BaggingDT          0.840328      0.81997      0.819767   \n",
       "AdaBoost           0.591089     0.574887      0.572674   \n",
       "ExtraTrees              1.0     0.998487           1.0   \n",
       "MLP_l1             0.997953     0.990923      0.992733   \n",
       "MLP_l2             0.998976     0.990923      0.988372   \n",
       "\n",
       "                                                    best_params  \n",
       "DecisionTree                                  {'max_depth': 14}  \n",
       "RandomForest              {'max_depth': 15, 'n_estimators': 55}  \n",
       "LogisticR_L1                {'penalty': 'l1', 'solver': 'saga'}  \n",
       "LogisticR_L2           {'penalty': 'l2', 'solver': 'liblinear'}  \n",
       "LogisticR            {'penalty': 'none', 'solver': 'newton-cg'}  \n",
       "RidgeClf                                                     {}  \n",
       "SVC_linear                       {'C': 0.5, 'kernel': 'linear'}  \n",
       "SVC_poly      {'C': 2.5, 'degree': 3, 'gamma': 'auto', 'kern...  \n",
       "SVC_others        {'C': 1.0, 'gamma': 'scale', 'kernel': 'rbf'}  \n",
       "GussianNB                                                    {}  \n",
       "KNN                                          {'n_neighbors': 3}  \n",
       "Bagging_SVC   {'base_estimator': SVC(kernel='linear'), 'n_es...  \n",
       "BaggingDT     {'base_estimator': DecisionTreeClassifier(max_...  \n",
       "AdaBoost      {'base_estimator': DecisionTreeClassifier(max_...  \n",
       "ExtraTrees                {'max_depth': 15, 'n_estimators': 65}  \n",
       "MLP_l1        {'activation': 'tanh', 'early_stopping': True,...  \n",
       "MLP_l2        {'activation': 'relu', 'early_stopping': True,...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compneuro",
   "language": "python",
   "name": "compneuro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
