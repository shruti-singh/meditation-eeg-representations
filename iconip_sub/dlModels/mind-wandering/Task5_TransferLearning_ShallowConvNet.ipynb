{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "\n",
    "# EEGNet-specific imports\n",
    "from EEGModels import ShallowConvNet\n",
    "from tensorflow.keras import utils as np_utils\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# PyRiemann imports\n",
    "from pyriemann.estimation import XdawnCovariances\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "from pyriemann.utils.viz import plot_confusion_matrix\n",
    "\n",
    "# # sklearn imports\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import classification_report, accuracy_score, f1_score, balanced_accuracy_score\n",
    "\n",
    "# # tools for plotting confusion matrices\n",
    "# import matplotlib\n",
    "# from matplotlib import pyplot as plt\n",
    "# import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_ctrvsone_and_transfer(user_split = True, runidx=1, data_type='mind-wandering', one_class='HT'):\n",
    "    X_train = None\n",
    "    y_train_medid = None\n",
    "    y_train_subjid = None\n",
    "    X_val = None\n",
    "    y_val_medid = None\n",
    "    y_val_subjid = None\n",
    "    X_test = None\n",
    "    y_test_medid = None\n",
    "    y_test_subjid = None\n",
    "\n",
    "    dtype_names = [\"HT\", \"SNY\", \"VIP\", \"CTR\"]\n",
    "\n",
    "    # user_split: Determines the creation of train/val/test set. In user_split, test/val users are never seen during the train. time_split randomly splits each user's chunks into train/val/test.\n",
    "    if user_split:\n",
    "        data_file_path = '../../iconip_data/{}/user_based_splits_with_timestamp_RUN{}.pkl'.format(data_type, runidx)\n",
    "    else:\n",
    "        data_file_path = '../../iconip_data/{}/time_based_splits_with_timestamp_RUN{}.pkl'.format(data_type, runidx)\n",
    "\n",
    "    with open(data_file_path, 'rb') as f:\n",
    "        all_data_splits = pickle.load(f)\n",
    "\n",
    "        data_index_label_oneclass = dtype_names.index(one_class)\n",
    "        data_index_label_ctr = dtype_names.index('CTR')\n",
    "        transfer_index_labels = [x for x in list(set([0,1,2,3]).difference(set([dtype_names.index(one_class), 3]))) ]\n",
    "        \n",
    "        X_train = all_data_splits['train']['x']\n",
    "        y_train_medid = all_data_splits['train']['y_med']\n",
    "        y_train_subjid = all_data_splits['train']['y_subj']\n",
    "        y_train_ts = all_data_splits['train']['y_ts']\n",
    "        clf_config_subset_indices = np.where((y_train_medid == data_index_label_oneclass) | (y_train_medid == data_index_label_ctr))[0]\n",
    "        X_train = X_train[(clf_config_subset_indices)]\n",
    "        y_train_medid = y_train_medid[(clf_config_subset_indices)]\n",
    "        y_train_subjid = y_train_subjid[clf_config_subset_indices]\n",
    "        y_train_ts = y_train_ts[(clf_config_subset_indices)]\n",
    "\n",
    "        X_val = all_data_splits['val']['x']\n",
    "        y_val_medid = all_data_splits['val']['y_med']\n",
    "        y_val_subjid = all_data_splits['val']['y_subj']\n",
    "        y_val_ts = all_data_splits['val']['y_ts']\n",
    "        clf_config_subset_indices = np.where((y_val_medid == data_index_label_oneclass) | (y_val_medid == data_index_label_ctr))[0]\n",
    "        X_val = X_val[(clf_config_subset_indices)]\n",
    "        y_val_medid = y_val_medid[(clf_config_subset_indices)]\n",
    "        y_val_subjid = y_val_subjid[(clf_config_subset_indices)]\n",
    "        y_val_ts = y_val_ts[(clf_config_subset_indices)]\n",
    "        \n",
    "        # Pretrain test and transfer test set\n",
    "        X_test = all_data_splits['test']['x']\n",
    "        y_test_medid = all_data_splits['test']['y_med']\n",
    "        y_test_subjid = all_data_splits['test']['y_subj']\n",
    "        y_test_ts = all_data_splits['test']['y_ts']\n",
    "        \n",
    "        # transfer test contains CTR from test and other two classes from the test.\n",
    "        ctr_othertwo_indices = np.where((y_test_medid == data_index_label_ctr) | (y_test_medid == transfer_index_labels[0]) | (y_test_medid == transfer_index_labels[1]))[0]\n",
    "        X_test_transfer = X_test[(ctr_othertwo_indices)]\n",
    "        y_test_transfer_medid = y_test_medid[(ctr_othertwo_indices)]\n",
    "        y_test_transfer_subjid = y_test_subjid[(ctr_othertwo_indices)]\n",
    "        \n",
    "        # pretrain test filter CTR and one_class from the test.\n",
    "        clf_config_subset_indices = np.where((y_test_medid == data_index_label_oneclass) | (y_test_medid == data_index_label_ctr))[0]\n",
    "        X_test = X_test[(clf_config_subset_indices)]\n",
    "        y_test_medid = y_test_medid[(clf_config_subset_indices)]\n",
    "        y_test_subjid = y_test_subjid[(clf_config_subset_indices)]\n",
    "        y_test_ts = y_test_ts[(clf_config_subset_indices)]\n",
    "\n",
    "    return X_train, y_train_medid, y_train_subjid, y_train_ts, X_val, y_val_medid, y_val_subjid, y_val_ts, X_test, y_test_medid, y_test_subjid, y_test_ts, X_test_transfer, y_test_transfer_medid, y_test_transfer_subjid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(user_split, med_tech_clf, data_type, one_class):\n",
    "    \n",
    "    kernels, chans, samples = 1, 64, 2560\n",
    "    \n",
    "    split_strat = 'user' if user_split else 'time'\n",
    "    model_results = pd.DataFrame()\n",
    "\n",
    "    for runidx in range(1, 6):\n",
    "        model_results['{}_Train_Acc'.format(runidx)] = None\n",
    "        model_results['{}_Val_Acc'.format(runidx)] = None\n",
    "        model_results['{}_Test_Acc'.format(runidx)] = None\n",
    "        model_results['{}_Test_Transfer_Acc'.format(runidx)] = None\n",
    "        model_results['{}_best_params'.format(runidx)] = None\n",
    "\n",
    "\n",
    "    for runidx in range(1, 6):\n",
    "        print(\"RUN ID: \", runidx)\n",
    "        med_tech_clf = True\n",
    "        \n",
    "        X_train, y_train_medid, y_train_subjid, y_train_ts, X_val, y_val_medid, y_val_subjid, y_val_ts, X_test, y_test_medid, y_test_subjid, y_test_ts, X_transfer, y_test_transfer_medid, y_test_transfer_subjid = load_dataset_ctrvsone_and_transfer(user_split=user_split, runidx=runidx, data_type=data_type, one_class=one_class)\n",
    "        ## Convert data to binary classification task, i.e. meditation expert as class 0 vs control group as class 1\n",
    "        y_train_medid = y_train_medid//3\n",
    "        y_val_medid = y_val_medid//3\n",
    "        y_test_medid = y_test_medid//3\n",
    "        y_test_transfer_medid = y_test_transfer_medid//3\n",
    "\n",
    "        best_clf_ours = None\n",
    "        best_clf_val = 0\n",
    "\n",
    "        #X_test_transfer, y_test_transfer_medid, y_test_transfer_subjid\n",
    "        if med_tech_clf:\n",
    "            Y_train      = y_train_medid\n",
    "            Y_validate   = y_val_medid\n",
    "            Y_test       = y_test_medid\n",
    "            Y_transfer   = y_test_transfer_medid\n",
    "            ckpt_file_suffix = '{}_med_clf'.format(split_strat)\n",
    "        else:\n",
    "            Y_train      = y_train_subjid\n",
    "            Y_validate   = y_val_subjid\n",
    "            Y_test       = y_test_subjid\n",
    "            Y_transfer   = y_test_transfer_subjid\n",
    "            ckpt_file_suffix = '{}_subj_clf'.format(split_strat)\n",
    "        nb_classes = np.unique(Y_train).shape[0]\n",
    "        print(np.unique(Y_train), \": unique class labels.\")\n",
    "    \n",
    "        ############################# EEGNet portion ##################################\n",
    "        # convert labels to one-hot encodings.\n",
    "        Y_train      = np_utils.to_categorical(Y_train)\n",
    "        Y_validate   = np_utils.to_categorical(Y_validate)\n",
    "        Y_test       = np_utils.to_categorical(Y_test)\n",
    "        Y_transfer   = np_utils.to_categorical(Y_transfer)\n",
    "\n",
    "        # convert data to NHWC (trials, channels, samples, kernels) format. Data \n",
    "        # contains 60 channels and 151 time-points. Set the number of kernels to 1.\n",
    "        X_train      = X_train.reshape(X_train.shape[0], chans, samples, kernels)\n",
    "        X_val        = X_val.reshape(X_val.shape[0], chans, samples, kernels)\n",
    "        X_test       = X_test.reshape(X_test.shape[0], chans, samples, kernels)\n",
    "        X_transfer   = X_transfer.reshape(X_transfer.shape[0], chans, samples, kernels)\n",
    "\n",
    "        print('X_train shape:', X_train.shape)\n",
    "        print(X_train.shape[0], 'train samples')\n",
    "        print(X_test.shape[0], 'test samples')\n",
    "        print(X_transfer.shape[0], 'transfer samples')\n",
    "        print(nb_classes, \" number of classes\")\n",
    "    \n",
    "        # configure the EEGNet-8,2,16 model with kernel length of 32 samples (other \n",
    "        # model configurations may do better, but this is a good starting point)\n",
    "        model = ShallowConvNet(nb_classes = nb_classes, Chans = chans, Samples = samples, \n",
    "                    dropoutRate = 0.5)\n",
    "\n",
    "        # compile the model and set the optimizers\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
    "                    metrics = ['accuracy'])\n",
    "\n",
    "        # count number of parameters in the model\n",
    "        numParams    = model.count_params()    \n",
    "\n",
    "        # set a valid path for your system to record model checkpoints\n",
    "        checkpointer = ModelCheckpoint(filepath='./ckpt/shallowconv/checkpoint_{}.h5'.format(ckpt_file_suffix), \n",
    "                                    verbose=1, save_best_only=True)\n",
    "        \n",
    "        ###############################################################################\n",
    "        # if the classification task was imbalanced (significantly more trials in one\n",
    "        # class versus the others) you can assign a weight to each class during \n",
    "        # optimization to balance it out. This data is approximately balanced so we \n",
    "        # don't need to do this, but is shown here for illustration/completeness. \n",
    "        ###############################################################################\n",
    "\n",
    "        # the syntax is {class_1:weight_1, class_2:weight_2,...}. Here just setting\n",
    "        # the weights all to be 1\n",
    "        class_weights = {cl: 1 for cl in range(nb_classes)}\n",
    "\n",
    "        ################################################################################\n",
    "        # fit the model. Due to very small sample sizes this can get\n",
    "        # pretty noisy run-to-run, but most runs should be comparable to xDAWN + \n",
    "        # Riemannian geometry classification (below)\n",
    "        ################################################################################\n",
    "        escallback = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=3)\n",
    "        fittedModel = model.fit(X_train, Y_train, batch_size = 128, epochs = 30, \n",
    "                                verbose = 2, validation_data=(X_val, Y_validate),\n",
    "                                callbacks=[checkpointer, escallback], class_weight = class_weights)\n",
    "\n",
    "        # load optimal weights\n",
    "        model.load_weights('./ckpt/shallowconv/checkpoint_{}.h5'.format(ckpt_file_suffix))\n",
    "    \n",
    "        probs       = model.predict(X_test)\n",
    "        preds       = probs.argmax(axis = -1)\n",
    "\n",
    "        probs_transfer = model.predict(X_transfer)\n",
    "        preds_transfer = probs_transfer.argmax(axis = -1)\n",
    "        \n",
    "        # save preds on train and val set for error analysis\n",
    "        preds_train_eegnet = model.predict(X_train)\n",
    "        preds_train_eegnet = preds_train_eegnet.argmax(axis = -1)\n",
    "        preds_val_eegnet = model.predict(X_val)\n",
    "        preds_val_eegnet = preds_val_eegnet.argmax(axis = -1)\n",
    "        \n",
    "        \n",
    "        acc_test         = np.mean(preds == Y_test.argmax(axis=-1))\n",
    "        acc_transfer     = np.mean(preds_transfer == Y_transfer.argmax(axis=-1))\n",
    "        print(\"ShallowConv Classification accuracy test and transfer: %f %f\" % (acc_test, acc_transfer))\n",
    "        model_results.loc[\"ShallowConv\", ['{}_Train_Acc'.format(runidx), '{}_Val_Acc'.format(runidx), '{}_Test_Acc'.format(runidx), '{}_Test_Transfer_Acc'.format(runidx)]] = [preds_train_eegnet, preds_val_eegnet, acc_test, acc_transfer]\n",
    "    if user_split:\n",
    "        model_results.to_csv('./RESULTS_T5{}_MED_TransferPRED_{}_CTRvs{}_ShallowConv.csv'.format('USER', data_type, one_class))\n",
    "    else:\n",
    "        model_results.to_csv('./RESULTS_T5_MED_TransferPRED_{}_CTRvs{}_ShallowConv.csv'.format(data_type, one_class))\n",
    "\n",
    "    return model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN ID:  1\n",
      "[0 1] : unique class labels.\n",
      "X_train shape: (983, 64, 2560, 1)\n",
      "983 train samples\n",
      "348 test samples\n",
      "511 transfer samples\n",
      "2  number of classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-23 23:56:26.655542: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.59126, saving model to ./ckpt/shallowconv/checkpoint_time_med_clf.h5\n",
      "8/8 - 50s - loss: 2.7537 - accuracy: 0.5748 - val_loss: 1.5913 - val_accuracy: 0.6228 - 50s/epoch - 6s/step\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.59126 to 0.26879, saving model to ./ckpt/shallowconv/checkpoint_time_med_clf.h5\n",
      "8/8 - 46s - loss: 0.8354 - accuracy: 0.7213 - val_loss: 0.2688 - val_accuracy: 0.8832 - 46s/epoch - 6s/step\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.26879\n",
      "8/8 - 45s - loss: 0.3555 - accuracy: 0.8566 - val_loss: 0.3625 - val_accuracy: 0.8204 - 45s/epoch - 6s/step\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.26879 to 0.17699, saving model to ./ckpt/shallowconv/checkpoint_time_med_clf.h5\n",
      "8/8 - 45s - loss: 0.2146 - accuracy: 0.9074 - val_loss: 0.1770 - val_accuracy: 0.9311 - 45s/epoch - 6s/step\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.17699\n",
      "8/8 - 46s - loss: 0.3334 - accuracy: 0.9003 - val_loss: 0.9527 - val_accuracy: 0.7874 - 46s/epoch - 6s/step\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.17699\n",
      "8/8 - 45s - loss: 0.6191 - accuracy: 0.8433 - val_loss: 0.6407 - val_accuracy: 0.8234 - 45s/epoch - 6s/step\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.17699 to 0.15000, saving model to ./ckpt/shallowconv/checkpoint_time_med_clf.h5\n",
      "8/8 - 44s - loss: 0.2727 - accuracy: 0.8932 - val_loss: 0.1500 - val_accuracy: 0.9401 - 44s/epoch - 5s/step\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.15000 to 0.14758, saving model to ./ckpt/shallowconv/checkpoint_time_med_clf.h5\n",
      "8/8 - 46s - loss: 0.1454 - accuracy: 0.9542 - val_loss: 0.1476 - val_accuracy: 0.9371 - 46s/epoch - 6s/step\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.14758 to 0.08103, saving model to ./ckpt/shallowconv/checkpoint_time_med_clf.h5\n",
      "8/8 - 45s - loss: 0.0573 - accuracy: 0.9756 - val_loss: 0.0810 - val_accuracy: 0.9731 - 45s/epoch - 6s/step\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.08103\n",
      "8/8 - 45s - loss: 0.0424 - accuracy: 0.9888 - val_loss: 0.0859 - val_accuracy: 0.9551 - 45s/epoch - 6s/step\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.08103\n",
      "8/8 - 42s - loss: 0.0566 - accuracy: 0.9766 - val_loss: 0.1129 - val_accuracy: 0.9611 - 42s/epoch - 5s/step\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.08103 to 0.07783, saving model to ./ckpt/shallowconv/checkpoint_time_med_clf.h5\n",
      "8/8 - 45s - loss: 0.0274 - accuracy: 0.9949 - val_loss: 0.0778 - val_accuracy: 0.9701 - 45s/epoch - 6s/step\n",
      "11/11 [==============================] - 3s 226ms/step\n",
      "16/16 [==============================] - 4s 236ms/step\n",
      "31/31 [==============================] - 7s 235ms/step\n",
      "11/11 [==============================] - 3s 223ms/step\n",
      "ShallowConv Classification accuracy test and transfer: 0.982759 0.724070\n",
      "RUN ID:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/singh_shruti/anaconda3/envs/compneuro/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3199: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return asarray(a).ndim\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1] : unique class labels.\n",
      "X_train shape: (983, 64, 2560, 1)\n",
      "983 train samples\n",
      "348 test samples\n",
      "511 transfer samples\n",
      "2  number of classes\n",
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.54732, saving model to ./ckpt/shallowconv/checkpoint_time_med_clf.h5\n",
      "8/8 - 45s - loss: 1.9559 - accuracy: 0.5961 - val_loss: 1.5473 - val_accuracy: 0.5838 - 45s/epoch - 6s/step\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.54732 to 0.39066, saving model to ./ckpt/shallowconv/checkpoint_time_med_clf.h5\n",
      "8/8 - 44s - loss: 0.6395 - accuracy: 0.7986 - val_loss: 0.3907 - val_accuracy: 0.8563 - 44s/epoch - 6s/step\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 0.39066 to 0.22642, saving model to ./ckpt/shallowconv/checkpoint_time_med_clf.h5\n",
      "8/8 - 44s - loss: 0.2939 - accuracy: 0.8749 - val_loss: 0.2264 - val_accuracy: 0.8982 - 44s/epoch - 5s/step\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.22642 to 0.20427, saving model to ./ckpt/shallowconv/checkpoint_time_med_clf.h5\n",
      "8/8 - 45s - loss: 0.1957 - accuracy: 0.9257 - val_loss: 0.2043 - val_accuracy: 0.9162 - 45s/epoch - 6s/step\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.20427 to 0.17604, saving model to ./ckpt/shallowconv/checkpoint_time_med_clf.h5\n",
      "8/8 - 44s - loss: 0.1126 - accuracy: 0.9573 - val_loss: 0.1760 - val_accuracy: 0.9072 - 44s/epoch - 6s/step\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.17604 to 0.13063, saving model to ./ckpt/shallowconv/checkpoint_time_med_clf.h5\n",
      "8/8 - 43s - loss: 0.0694 - accuracy: 0.9746 - val_loss: 0.1306 - val_accuracy: 0.9371 - 43s/epoch - 5s/step\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.13063 to 0.10783, saving model to ./ckpt/shallowconv/checkpoint_time_med_clf.h5\n",
      "8/8 - 43s - loss: 0.0592 - accuracy: 0.9776 - val_loss: 0.1078 - val_accuracy: 0.9581 - 43s/epoch - 5s/step\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.10783\n",
      "8/8 - 42s - loss: 0.0608 - accuracy: 0.9807 - val_loss: 0.1260 - val_accuracy: 0.9431 - 42s/epoch - 5s/step\n",
      "Epoch 9/30\n"
     ]
    }
   ],
   "source": [
    "# time split\n",
    "for one_class in ['HT', 'SNY', 'VIP']:\n",
    "    model_results = train_model(user_split=False, med_tech_clf=True, data_type='mind-wandering', one_class=one_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User split\n",
    "for one_class in ['HT', 'SNY', 'VIP']:\n",
    "    model_results = train_model(user_split=True, med_tech_clf=True, data_type='mind-wandering', one_class=one_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "53b83f552ae48d324223bc29fa14bcaf581b7fdcc2f8122e8d13756a70e1d54e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
